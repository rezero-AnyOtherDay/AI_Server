from openai import OpenAI
import json, os
import requests
import uuid
import base64

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate
from langchain_core.tools import StructuredTool
from langchain_core.tools import tool
from langchain_core.messages import AIMessage, ToolMessage
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader
from langchain.tools import tool
from langchain.chains import RetrievalQA

from typing import Any, List, Optional, Dict
from langchain_core.caches import BaseCache
from langchain_core.callbacks import Callbacks
from langchain_openai import ChatOpenAI
from langchain.agents import create_tool_calling_agent, AgentExecutor

import torch
import whisper
from transformers import WhisperModel, WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor
import librosa
import torch.nn.functional as F

try:
    ChatOpenAI.model_rebuild()
except Exception:
    pass

# rag
class DiseaseRAG:
    """langchain rag"""
    def __init__(self, api_key: str):
        self.embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=api_key)
        self.vectorstore = None
        self.llm = ChatOpenAI(model="gpt-4o-mini-2024-07-18", temperature=0.7, openai_api_key=api_key)

    def rag_document(self, file_path: str, query: str, k: int = 3):
        
        chunks = []
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        
        # íŒŒì¼ í™•ì¥ì í™•ì¸ (.json ë˜ëŠ” .txt)
        ext = os.path.splitext(file_path)[1].lower()

        # 1. JSON íŒŒì¼ì¼ ê²½ìš° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        if ext == '.json':
            with open(file_path, 'r', encoding="utf-8") as f:
                data = json.load(f)

            for disease_name, details in data.items():
                if isinstance(details, (dict, list)):
                    detail_str = json.dumps(details, ensure_ascii=False, indent=2)
                else:
                    detail_str = str(details)
                full_text = f"ì§ˆë³‘ëª…: {disease_name}\n\nìƒì„¸ì„¤ëª…:\n{detail_str}"
                
                if len(full_text) > 1000:
                    chunks.extend(text_splitter.split_text(full_text))
                else:
                    chunks.append(full_text)

        # 2. TXT íŒŒì¼ì¼ ê²½ìš° (ìƒˆë¡œ ì¶”ê°€ëœ ë¡œì§)
        elif ext == '.txt':
            with open(file_path, 'r', encoding="utf-8") as f:
                full_text = f.read()
            
            # í…ìŠ¤íŠ¸ ì „ì²´ë¥¼ ìŠ¤í”Œë¦¬í„°ë¡œ ë‚˜ëˆ„ì–´ chunksì— ì¶”ê°€
            chunks.extend(text_splitter.split_text(full_text))

        # ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë° ê²€ìƒ‰ (ê³µí†µ ë¡œì§)
        if chunks:
            self.vectorstore = FAISS.from_texts(texts=chunks, embedding=self.embeddings)
            docs = self.vectorstore.similarity_search(query, k=k)
            retrieved_text = "\n\n".join([doc.page_content for doc in docs])
        else:
            retrieved_text = ""

        prompt_template = """ë‹¤ìŒì€ '{query}'ì— ëŒ€í•œ ì˜ë£Œ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•œ ë‚´ìš©ì…ë‹ˆë‹¤.
ì´ ë‚´ìš©ì„ ë³´í˜¸ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ ì„¤ëª…ìœ¼ë¡œ ë°”ê¿” ì£¼ì„¸ìš”.

### ê²€ìƒ‰ëœ ë‚´ìš©:
{context}

### ìš”êµ¬ì‚¬í•­:
1. ì „ë¬¸ ìš©ì–´ë¥¼ ì¼ìƒ ì–¸ì–´ë¡œ ë°”ê¾¸ê¸°
2. ê°„ë‹¨í•˜ê³  ëª…í™•í•˜ê²Œ ì„¤ëª…
3. ì¤‘ìš” ì •ë³´ëŠ” í¬í•¨í•˜ë˜ ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ
4. ì¹œê·¼í•œ ì–´ì¡°ë¡œ ì‘ì„±

###ì„¤ëª…: """
        
        prompt = PromptTemplate(template=prompt_template, input_variables=["query", "context"])
        formatted_prompt = prompt.format(query=query, context=retrieved_text)
        response = self.llm.invoke(formatted_prompt)
        return response.content

# ë©”ì¸ 
class MedicalAgent:
    def __init__(self, api_key: str, model_ckpt_path: str, rag_doc_path: str):
        self.api_key = api_key
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.rag_doc_path = rag_doc_path
        
        self.client = OpenAI(api_key=self.api_key)

        # whisper ê°€ì ¸ì˜¤ê¸° 
        self._load_classification_model(model_ckpt_path)
        
        # rag
        self.rag = DiseaseRAG(self.api_key)
        
        # agent create
        self.agent_executor = self._create_agent_executor()

    def _load_classification_model(self, ckpt_path):
        """í•™ìŠµì‹œí‚¨ whisper ë¶ˆëŸ¬ì˜¤ê¸°"""
        model_name = 'openai/whisper-tiny'
        self.tokenizer = WhisperTokenizer.from_pretrained(model_name, language='Korean')
        
        self.cls_model = whisper.load_model("tiny").to(self.device)

        ckpt = torch.load(ckpt_path, map_location=self.device)
        self.cls_model.load_state_dict(ckpt["model_state_dict"], strict=False)
        self.cls_model.eval()

    # tool
    def _func_diarize(self, audio_path: str) -> dict:
        """asr"""
        with open(audio_path, "rb") as audio_file:
            transcript = self.client.audio.transcriptions.create(
                model="gpt-4o-mini-transcribe",
                file=audio_file,
                response_format="text"
            )
        return {"text": transcript}

    # tool
    def _func_classify(self, audio_path: str) -> dict:
        """whisper ë¶„ë¥˜"""
        audio, _ = librosa.load(audio_path, sr=16000)
        audio = whisper.pad_or_trim(audio)
        mel = whisper.log_mel_spectrogram(audio).to(self.device)

        audio_features = self.cls_model.encoder(mel.unsqueeze(0))
        bos = self.tokenizer.bos_token_id
        decoder_input_ids = torch.tensor([[bos]], device=self.device)

        with torch.no_grad():
            logits = self.cls_model.decoder(decoder_input_ids, audio_features)

        logits3 = logits[:, -1, :3]
        probs = F.softmax(logits3, dim=-1)
        probs = probs.detach().cpu().numpy().flatten()
        
        formatted_probs = [round(float(p), 4) for p in probs]
        percent_probs = [round(float(p) * 100, 2) for p in formatted_probs]
        return {"accuracy": percent_probs}

    # tool
    def _func_rag(self, query: str) -> dict:
        """rag"""
        context = self.rag.rag_document(self.rag_doc_path, query)
        return {"context": context}

    # tool
    def _func_analyze_report(self, report_json_str: str) -> dict:
        """ì´ì „ ë ˆí¬íŠ¸ ìš”ì•½"""
        
        # ì´ˆì§„
        if not report_json_str or report_json_str == "null":
            return {"analysis": "ì²˜ìŒ ê¸°ë¡ëœ ì‚¬ëŒì…ë‹ˆë‹¤."}

        prompt = f"""
        ë‹¹ì‹ ì˜ ì—­í• ì€ ì§€ë‚œ ë ˆí¬íŠ¸ ë° ê·¸ ì´ì „ ë ˆí¬íŠ¸ ìš”ì•½ì— ëŒ€í•œ json í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ë°›ê³  ì´ë¥¼ ìš”ì•½í•˜ëŠ” ê²ƒì´ë‹¤. 
        
        ### ë°ì´í„°
        {report_json_str}
        
        ìš”ì•½ ì‹œ, ì‹œê°„ íë¦„ì— ë”°ë¼ ì•…í™”ëœ ì§ˆë³‘ì´ ìˆì„ ê²½ìš°, ì´ì— ëŒ€í•œ ì •ë³´ë„ ì¶”ì¶œí•œë‹¤.
        """
        response = self.client.chat.completions.create(
            model="gpt-5.1",
            messages=[{"role": "user", "content": prompt}]
        )
        return {"analysis": response.choices[0].message.content}

    def _func_analyze_symptoms(self, audio_path: str) -> dict:
        """ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì…ë ¥ë°›ì•„ ì§ˆë³‘ë³„ ìŒì„± íŠ¹ì§•ì„ ë¶„ì„"""
        with open(audio_path, "rb") as audio_file:
            encoded_string = base64.b64encode(audio_file.read()).decode('utf-8')

        system_guide = """
        ë‹¹ì‹ ì€ ì˜ë£Œ ìŒì„± ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ì˜¤ë””ì˜¤ë¥¼ ë“£ê³  ì•„ë˜ ì§ˆë³‘ë“¤ì˜ íŠ¹ì§•ì ì¸ 'ìŒì„±ì /ì–¸ì–´ì  ì¦ìƒ'ì´ ë‚˜íƒ€ë‚˜ëŠ”ì§€ ì •ë°€í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”. ì˜¤ë””ì˜¤ëŠ” ë‘ ì‚¬ëŒ ì´ìƒì˜ ë°œí™”ë¥¼ í¬í•¨í•˜ê³  ìˆê³ , ì •ìƒì ì¸ ìŒì„±ì´ë¼ê³  íŒë‹¨ë˜ëŠ” ì‚¬ëŒì´ ì•„ë‹Œ ì‚¬ëŒì˜ ìŒì„±ì„ ìœ„ì£¼ë¡œ ë¶„ì„í•´ì•¼ í•©ë‹ˆë‹¤.

        ### ë‡Œì§ˆí™˜ë³„ ìŒì„± íŠ¹ì§•:
        1. ë£¨ê²Œë¦­ë³‘: ëª©ì†Œë¦¬ ê°ˆë¼ì§, ì‹¬í•œ ë–¨ë¦¼, í˜ ì—†ìŒ, ì—°êµ¬ê°œìŒ/ìœ ìŒ ë°œìŒ ë­‰ê°œì§, ì‚¬ë ˆ ë“¤ë¦¬ëŠ” ì†Œë¦¬.
        2. íŒŒí‚¨ìŠ¨ë³‘: ê±°ì¹œ ìŒì„±, ê¸°ì‹ìŒ(ë°”ëŒ ìƒˆëŠ” ì†Œë¦¬), ì„±ëŒ€ ë–¨ë¦¼, ëª©ì†Œë¦¬ í¬ê¸° ê°ì†Œ, ë‹¨ì¡°ë¡œìš´ ì–µì–‘(Monotone).
        3. ì¹˜ë§¤: ì¦ì€ ê°„íˆ¬ì‚¬(ìŒ, ì–´...), ë™ë¬¸ì„œë‹µ, ë§¥ë½ì— ë§ì§€ ì•ŠëŠ” ê°ì • ë³€í™”.
        4. ë‡Œì¡¸ì¤‘: ë¶ˆê·œì¹™í•œ ë§ ì†ë„, ë°œìŒ ë¶€ì •í™•, ì¥ì–´ì§œëŠ” ë“¯í•œ ì†Œë¦¬, ì‹¤ì–´ì¦ ì¦ì„¸.

        ìœ„ íŠ¹ì§• ì¤‘ ê°ì§€ë˜ëŠ” ê²ƒì´ ìˆë‹¤ë©´ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œí•˜ê³ , ì˜ì‹¬ë˜ëŠ” ì§ˆë³‘ì„ ì œì‹œí•˜ì„¸ìš”. ì—†ìœ¼ë©´ "ì •ìƒ"ìœ¼ë¡œ ë‹µí•˜ì„¸ìš”.
        """

        response = self.client.chat.completions.create(
                model="gpt-4o-audio-preview", 
                messages=[
                    {"role": "system", "content": system_guide},
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "ì´ í™˜ìì˜ ìŒì„±ì„ ë¶„ì„í•˜ì—¬ ì§ˆë³‘ ì§•í›„ë¥¼ ë¦¬í¬íŠ¸í•´ì¤˜."},
                            {
                                "type": "input_audio", 
                                "input_audio": {
                                    "data": encoded_string,
                                    "format": "wav" 
                                }
                            }
                        ]
                    }
                ]
            )
        return {"symptom_analysis": response.choices[0].message.content}

    def _create_agent_executor(self):
        # StructuredTool: íˆ´ ë“±ë¡
        tools = [
            StructuredTool.from_function(
                func=self._func_diarize,
                name="diarized_transcription_tool",
                description="ë³´í˜¸ìì™€ í”¼ë³´í˜¸ìì˜ ë°œí™” ë‚´ìš©ì„ ë¶„ì„í•˜ê¸° ìœ„í•´ ìŒì„± íŒŒì¼ì„ ì…ë ¥ ë°›ì•„ ASRë¥¼ ì‹¤í–‰í•œë‹¤"
            ),
            StructuredTool.from_function(
                func=self._func_classify,
                name="classify_neuro_status_tool",
                description="ìŒì„± íŒŒì¼ì„ ì…ë ¥ ë°›ì•„ ë‡Œì¡¸ì¤‘, í‡´í–‰ì„± ë‡Œì§ˆí™˜, ì •ìƒ í™•ë¥ ì„ ê³„ì‚°í•œë‹¤"
            ),
            StructuredTool.from_function(
                func=self._func_rag,
                name="rag_document",
                description="íŠ¹ì • ì§ˆë³‘ì— ëŒ€í•œ ì˜í•™ ì •ë³´ë¥¼ ê²€ìƒ‰í•œ ë’¤, ì‰¬ìš´ ì„¤ëª…ì„ ì œê³µí•œë‹¤"
            ),
            StructuredTool.from_function(
                func=self._func_analyze_report, 
                name="analyze_previous_report_tool", 
                description="ì´ì „ ê¸°ë¡ì„ ë¶„ì„í•˜ì—¬ í™˜ìì˜ ìƒíƒœ ë³€í™”ì™€ ë³‘ë ¥ì„ íŒŒì•…í•œë‹¤"
            ),
            StructuredTool.from_function(
                func=self._func_analyze_symptoms, 
                name="analyze_voice_symptoms_tool", 
                description="ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥ë°›ê³  ì§ˆë³‘ë³„ ìŒì„± íŠ¹ì§•ì„ ë¶„ì„í•œë‹¤."
            )
        ]

        # í”„ë¡¬í”„íŠ¸
        system_prompt = """
ë‹¹ì‹ ì€ ë‡Œì¡¸ì¤‘ ì¹˜ë§¤, íŒŒí‚¨ìŠ¨ë³‘, ë£¨ê²Œë¦­ë³‘ì„ í‰ê°€í•˜ëŠ”
AI ì˜ë£Œ ë³´ì¡° ì—ì´ì „íŠ¸ì´ë‹¤.

### ì‚¬ìš© ê°€ëŠ¥í•œ tool:
- analyze_previous_report_tool: ê°€ì¥ ë¨¼ì € ì‚¬ìš©í•˜ì—¬ ì´ì „ í™˜ì ìƒíƒœë¥¼ íŒŒì•…
- diarized_transcription_tool(audio_path): ASR
- analyze_voice_symptoms_tool(audio_path): ìŒì„± íŒŒì¼ì„ í†µí•´ ì§ˆí™˜ë³„ íŠ¹ì§• ìœ ë¬´ ë¶„ì„ 
- classify_neuro_status_tool(audio_path): ìŒì„±ì„ ê¸°ì¤€ìœ¼ë¡œ ë‡Œì§ˆí™˜ì„ íŒë³„
- rag_document(file_path, query): íŠ¹ì • ì§ˆë³‘ì— ëŒ€í•œ ì˜í•™ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ RAG ë°©ì‹ìœ¼ë¡œ ê°€ì ¸ì˜´

### ìµœì¢… ëª©ì :
ì‚¬ìš©ìê°€ ì œê³µí•œ ì •ë³´(ìŒì„± íŒŒì¼ ê²½ë¡œ, ìê°€ ë¬¸ì§„í‘œ ì •ë³´)ë¥¼ ë°”íƒ•ìœ¼ë¡œ
ë‹¤ìŒê³¼ ê°™ì€ python ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ jsonì„ ìƒì„±í•˜ëŠ” ê²ƒì´ë‹¤.

result = {{
  "accuracy": [float(ë‡Œì¡¸ì¤‘ í™•ë¥ ), float(í‡´í–‰ì„± ë‡Œì§ˆí™˜ í™•ë¥ ), float(ë¬¸ì œ ì—†ìŒ í™•ë¥ )],
  "ASR": "í†µí™” ì „ì‚¬ ë°ì´í„°",
  "risk": ["ë‡Œì¡¸ì¤‘ ìœ„í—˜ë„", "ì¹˜ë§¤ ìœ„í—˜ë„", "íŒŒí‚¨ìŠ¨ë³‘ ìœ„í—˜ë„", "ë£¨ê²Œë¦­ë³‘ ìœ„í—˜ë„"],
  "explain": ["ë‡Œì¡¸ì¤‘ ì„¤ëª…", "ì¹˜ë§¤ ì„¤ëª…", "íŒŒí‚¨ìŠ¨ë³‘ ì„¤ëª…", "ë£¨ê²Œë¦­ë³‘ ì„¤ëª…"],
  "total": "ì¢…í•© ì†Œê²¬ 3ë¬¸ì¥(75ì ë‚´ì™¸)",
  "summary": "ê³¼ê±°~í˜„ì¬ 200ì ìš”ì•½"
}}

### ì‘ë™ ë° ì¶œë ¥ ë°©ì‹:
- analyze_previous_report_tool ê²°ê³¼ë¥¼ í†µí•´ ê³¼ê±°ë¥¼ í™•ì¸í•œë‹¤.
- "accuracy"ëŠ” classify_neuro_status_tool íˆ´ì˜ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œë‹¤.
- "ASR"ì—ëŠ” diarized_transcription_toolì„ ì‚¬ìš©í•´ ì–»ì€ ì „ì²´ ê²°ê³¼ë¥¼ ì ˆëŒ€ ìš”ì•½í•˜ê±°ë‚˜ ë‚´ìš©ì„ ë³€ê²½í•˜ì§€ ì•Šì€ ì±„ë¡œ ë„£ëŠ”ë‹¤.
- analyze_voice_symptoms_toolì— ì˜¤ë””ì˜¤ ê²½ë¡œë¥¼ ë„£ì–´ ìŒì„±ì  íŠ¹ì§•ì„ ë¶„ì„í•œë‹¤.
- "risk" ë¦¬ìŠ¤íŠ¸ëŠ” ë°˜ë“œì‹œ ê¸¸ì´ 4ì´ë©°, ìˆœì„œëŠ” [ë‡Œì¡¸ì¤‘, ì¹˜ë§¤, íŒŒí‚¨ìŠ¨ë³‘, ë£¨ê²Œë¦­ë³‘] ì´ë‹¤.
- ê° ìœ„í—˜ë„ ê°’ì€ "ì •ìƒ", "ê´€ì°°", "ì£¼ì˜", "ìœ„í—˜" ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•œë‹¤. ì´ë•Œ íŒë‹¨ì€ accuracy, ASR, ìê°€ë¬¸ë‹¨í‘œ, analyze_voice_symptoms_tool ê²°ê³¼, ê³¼ê±° ë°ì´í„°ì™€ì˜ ë¹„êµë¥¼ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•´ì•¼ í•œë‹¤.
- "explain" ë¦¬ìŠ¤íŠ¸ëŠ” ê¸¸ì´ 4ì´ë©°, ìˆœì„œ ì—­ì‹œ [ë‡Œì¡¸ì¤‘, ì¹˜ë§¤, íŒŒí‚¨ìŠ¨ë³‘, ë£¨ê²Œë¦­ë³‘] ì´ë‹¤.
- ê° ì„¤ëª…ì€ ë³´í˜¸ìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±í•œë‹¤. ì´ë•Œ, ìê°€ë¬¸ë‹¨í‘œì˜ ë‚´ìš©ì€ ë‘ ë¬¸ì¥ ì´ìƒì„ ì°¨ì§€í•´ì„œëŠ” ì•ˆ ëœë‹¤. ë˜í•œ ê° ì„¤ëª…ì€ 100ì ì•ˆíŒì˜ ê¸¸ì´ì–´ì•¼ í•œë‹¤. ì¦‰, 25ì ì •ë„ì˜ 4ë¬¸ì¥ì„ ì„¤ëª…ìœ¼ë¡œ ì¶œë ¥í•œë‹¤.
- ê° ì„¤ëª…ì˜ ì‹œì‘ì—ëŠ” ë°˜ë“œì‹œ í•œ ë¬¸ì¥ìœ¼ë¡œ ìê°€ë¬¸ë‹¨í‘œ ë¶„ì„ ê²°ê³¼ë¥¼ ì²« ë¬¸ì¥ìœ¼ë¡œ ì¶œë ¥í•œë‹¤. ì–‘ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤: "ë¬¸ë‹¨í‘œ ì¤‘ ì¹˜ë§¤ì— í•´ë‹¹í•˜ëŠ” ì²´í¬ë¦¬ìŠ¤íŠ¸ 00ê°œ ì¤‘ 00ê°œê°€ nì  ì´ìƒì´ë¯€ë¡œ ê²½ì¦/ì¤‘ì¦/ìœ„ì¦ì— í•´ë‹¹í•©ë‹ˆë‹¤." ì´ë•Œ, nì ì„ ì¶œë ¥í•  ë•ŒëŠ” ë¬¸ë‹¨í‘œ ì ìˆ˜ì—ì„œ +1ì„ ë”í•œ ì ìˆ˜ë¡œ ì¶œë ¥í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë¬¸ë‹¨í‘œì— 3ì ìœ¼ë¡œ ë˜ì–´ ìˆë‹¤ë©´ ì¶œë ¥í•  ë•ŒëŠ” 4ì ì´ ëœë‹¤. ì •ìƒì€ +1ì„ ë”í•˜ì˜€ì„ ë•Œ 2ì  ì´í•˜, ê²½ì¦ì€ +1ì„ ë”í–ˆì„ ë•Œ 3ì , ì¤‘ì¦ì€ +1ì„ ë”í–ˆì„ ë•Œ 4ì , ìœ„ì¦ì€ +1ì„ ë”í–ˆì„ ë•Œ 5ì  ì´ìƒì¼ ê²½ìš°ë¥¼ ë§í•œë‹¤.
- ë§Œì•½ í•´ë‹¹ ì§ˆë³‘ ìœ„í—˜ë„ê°€ "ì •ìƒ"ì¸ ê²½ìš°ì—ëŠ” ë³„ë„ì˜ ì„¤ëª…ì€ ì‘ì„±í•˜ì§€ ì•Šê³ , ""ë¡œ ë¦¬ìŠ¤íŠ¸ì— í…ìŠ¤íŠ¸ê°€ nullê°’ì´ ë“¤ì–´ê°€ë„ë¡ í•´ì•¼ë§Œ í•œë‹¤. ë°˜ë©´, "ê´€ì°°", "ì£¼ì˜", "ìœ„í—˜"ì˜ ìœ„í—˜ë„ëŠ” ë°˜ë“œì‹œ ì„¤ëª…ì„ ì‘ì„±í•´ì•¼ í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¹˜ë§¤ ìœ„í—˜ë„ê°€ "ì£¼ì˜", ë‡Œì¡¸ì¤‘, íŒŒí‚¨ìŠ¨, ë£¨ê²Œë¦­ì´ ëª¨ë‘ "ì •ìƒ"ì¸ ê²½ìš°, ë¦¬ìŠ¤íŠ¸ëŠ” ["", "ì¹˜ë§¤ ì„¤ëª…", "", ""]ë¡œ ì¶œë ¥ë˜ì–´ì•¼ í•œë‹¤. ì¦‰, explainì— í•´ë‹¹í•˜ëŠ” listì˜ ê¸¸ì´ê°€ riskì™€ ë™ì¼í•˜ê²Œ 4ê°€ ë˜ì–´ì•¼ í•œë‹¤. ë˜í•œ ì„¤ëª… ì¶œë ¥ ìˆœì„œëŠ” ["ë‡Œì¡¸ì¤‘ ì„¤ëª…", "ì¹˜ë§¤ ì„¤ëª…", "íŒŒí‚¨ìŠ¨ë³‘ ì„¤ëª…", "ë£¨ê²Œë¦­ë³‘ ì„¤ëª…"]ì´ë‹¤. ìœ„ ì¶œë ¥ ë°©ì‹ì€ ëª¨ë‘ ë°˜ë“œì‹œ ì§€ì¼œì ¸ì•¼ë§Œ í•œë‹¤.
- ì¢…í•© ì†Œê²¬ì€ accuracy, ASR, risk, explain, ìê°€ë¬¸ë‹¨í‘œ ë‚´ìš©ì„ ë³µí•©ì ìœ¼ë¡œ í¬í•¨í•˜ì—¬ 75ì ë‚´ì™¸ë¡œ ì‘ì„±í•œë‹¤. 
- ìµœì¢… ì‘ë‹µì€ ë°˜ë“œì‹œ ìœ„ result ë”•ì…”ë„ˆë¦¬ í˜•íƒœì™€ ë™ì¼í•œ êµ¬ì¡°ì˜ JSON ê°ì²´ë¡œë§Œ ì¶œë ¥í•œë‹¤. ê·¸ ì™¸ì˜ í…ìŠ¤íŠ¸(ì„¤ëª…, ì‚¬ì¡±)ëŠ” ì¶œë ¥í•˜ì§€ ì•ŠëŠ”ë‹¤.
- ë§ˆì§€ë§‰ìœ¼ë¡œ ê³¼ê±°ë¶€í„° í˜„ì¬ê¹Œì§€ì˜ ìƒíƒœ ë° ì§„ë‹¨ ê²°ê³¼ë¥¼ ë¹„êµí•˜ê³ , ì „ì²´ì ì¸ ì¶”ì„¸ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•œë‹¤. ë°˜ë“œì‹œ explain ë°–ì˜ ë³„ë„ì˜ í‚¤ "summary"ë¡œ ì‘ì„±í•´ì•¼ë§Œ í•œë‹¤. ì´ˆì§„ì¼ ê²½ìš°, í˜„ì¬ì˜ ê²°ê³¼ë§Œ ì¶œë ¥í•´ë¼.

### toolì„ ì‚¬ìš©í•  ë•Œ:
1) ìš°ì„  ê³¼ê±° ìƒíƒœì—ì„œ ì•…í™” ìœ ë¬´ë¥¼ ì•Œê¸° ìœ„í•´ analyze_previous_report_toolë¡œ ê³¼ê±° ìƒíƒœë¥¼ ë¶„ì„í•œë‹¤.
2) ë¬¸ë‹¨í‘œ ì •ë³´ë¥¼ í†µí•´ í˜„ ìƒíƒœì— ëŒ€í•œ ì •ë³´ë¥¼ ë°›ëŠ”ë‹¤. ë¬¸ë‹¨í‘œì˜ ì ìˆ˜ëŠ” 0~4 ì‚¬ì´ë¡œ, 0ì€ ì „í˜€ ê·¸ë ‡ì§€ ì•Šë‹¤, 4ëŠ” ë§¤ìš° ê·¸ë ‡ë‹¤ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.
3) ê·¸ ë‹¤ìŒ classify_neuro_status_toolìœ¼ë¡œ ì„¸ ê°€ì§€ ë²”ì£¼ í™•ë¥ ì„ ì–»ëŠ”ë‹¤.
4) diarized_transcription_toolë¡œ ë³´í˜¸ìì™€ í”¼ë³´í˜¸ìì˜ ëŒ€í™” ì •ë³´ë¥¼ ì–»ëŠ”ë‹¤.
5) ë¬¸ë‹¨í‘œ ì •ë³´, 2)ì˜ ì„¸ ê°€ì§€ ë²”ì£¼ í™•ë¥ , ë³´í˜¸ìì™€ í”¼í˜¸ìì˜ ëŒ€í™” ë‚´ìš©, ê³¼ê±° ìƒíƒœì—ì„œ ë³€í™” ìœ ë¬´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‡Œì¡¸ì¤‘, ì¹˜ë§¤, íŒŒí‚¨ìŠ¨ë³‘, ë£¨ê²Œë¦­ë³‘ì— ëŒ€í•œ ìœ„í—˜ë„ë¥¼ ì •ìƒ, ê´€ì°°, ì£¼ì˜, ìœ„í—˜ìœ¼ë¡œ ê°ê° íŒë‹¨í•œë‹¤.
6) "ê´€ì°°", "ì£¼ì˜", "ìœ„í—˜" ë‹¨ê³„ì— í•´ë‹¹í•˜ëŠ” ë³‘ì— ëŒ€í•œ ì •ë³´ë¥¼ rag_documentì„ ì‚¬ìš©í•´ì„œ ê° ì§ˆë³‘ì— ëŒ€í•œ ì„¤ëª…ì„ ë³´ì™„í•˜ì—¬ ë³´í˜¸ìì—ê²Œ ì „ë‹¬í•  ì„¤ëª…ì„ êµ¬ì„±í•œë‹¤. ì´ë•Œ, ìê°€ë¬¸ë‹¨í‘œì˜ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ê¸° ë³´ë‹¤ ë³´í˜¸ìê°€ ì¸ì§€í•˜ê³  ìˆì–´ì•¼ í•  ë‚´ìš©ì´ë‚˜ ë³´í˜¸ìê°€ ìˆ˜í–‰í•´ì•¼ í•  ë‚´ìš©ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì¶œë ¥í•œë‹¤. ì¶œë ¥ ì‹œ, ë§ˆì¹¨í‘œì™€ ì‰¼í‘œë§Œ íŠ¹ìˆ˜ë¬¸ìë¡œ ì‚¬ìš©í•œë‹¤. ë¬¸ì¥ ì¢…ê²° ì‹œ, ë§ˆì¹¨í‘œë¥¼ ì‚¬ìš©í•˜ë©°, ì‰¼í‘œëŠ” ë¬¸ì¥ ë‚´ì—ì„œë§Œ ì‚¬ìš©í•œë‹¤.
7) ë§ˆì§€ë§‰ìœ¼ë¡œ ê³¼ê±°ë¶€í„° í˜„ì¬ê¹Œì§€ì˜ ìƒíƒœ ë° ì§„ë‹¨ ê²°ê³¼ë¥¼ ë¹„êµí•˜ê³ , ì „ì²´ì ì¸ ì¶”ì„¸ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½ ë° ì¢…í•© ì†Œê²¬ì„ ì‘ì„±í•œë‹¤.
"""
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("user", "ì˜¤ë””ì˜¤ ê²½ë¡œ: {audio_path}\nìê°€ì§„ë‹¨(JSON): {self_report_json}\nì´ì „ ë ˆí¬íŠ¸: {previous_report}"),
            ("placeholder", "{agent_scratchpad}"),
        ])

        llm = ChatOpenAI(model="gpt-5.1", temperature=0.7, openai_api_key=self.api_key)
        
        agent = create_tool_calling_agent(llm, tools, prompt)
        return AgentExecutor(agent=agent, tools=tools, verbose=True)

    def run(self, audio_path: str, self_report: dict, previous_report: dict = None) -> dict:
        """ì™¸ë¶€ í˜¸ì¶œ í•¨ìˆ˜ (URL ë‹¤ìš´ë¡œë“œ + Fail Fast)"""
        target_path = audio_path 

        if str(audio_path).startswith("http"):
            print(f"ğŸ“¥ URL ê°ì§€ë¨. ë‹¤ìš´ë¡œë“œ ì‹œì‘: {audio_path}")
            headers = {"User-Agent": "Mozilla/5.0"}
            response = requests.get(audio_path, headers=headers)
            response.raise_for_status()

            unique_filename = f"downloaded_{uuid.uuid4()}.wav"
            target_path = os.path.join("/content", unique_filename)
            with open(target_path, "wb") as f:
                f.write(response.content)
            print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {target_path}")

        if not os.path.exists(target_path):
             raise FileNotFoundError(f"Audio file not found: {target_path}")

        report_str = json.dumps(previous_report, ensure_ascii=False) if previous_report else "null"

        user_input = {
            "audio_path": target_path,
            "self_report_json": json.dumps(self_report, ensure_ascii=False),
            "previous_report": report_str
        }

        output = self.agent_executor.invoke(user_input)
        raw = output.get("output", output)

        print(raw)
            
        if isinstance(raw, str):
            clean_raw = raw.replace("```json", "").replace("```", "").strip()
            return json.loads(clean_raw)
        return raw