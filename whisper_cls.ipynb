{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DBHlZEdRU3x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88cb2729-81c9-453d-d8b1-e2dd54644c04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install --upgrade jiwer evaluate"
      ],
      "metadata": {
        "id": "kpBg22JlWP7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, sys, re, math, random, time, json, pickle, gc\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim # 안 될 확률 높음\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import librosa, evaluate, jiwer\n",
        "from typing import Any, Dict, List, Union\n",
        "from transformers import WhisperModel, WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
        "\n",
        "import whisper\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# csv는 파일 경로(audio_file_path), classId(class를 숫자로 바꾼 거), class(한국어로 된 거)\n",
        "# 각각 경로 추가하기\n",
        "\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/코딩/새싹해커톤/audio_cls_dataset.csv')\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/코딩/새싹해커톤/normal_audio_dataset.csv')\n",
        "\n",
        "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
        "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "# combined_df.to_csv(\"/content/drive/MyDrive/코딩/새싹해커톤/combined_audio_dataset.csv\", index=False, encoding='utf-8-sig')\n",
        "\n",
        "train_df, temp_df = train_test_split(\n",
        "    combined_df,\n",
        "    train_size=int(len(combined_df) * 0.7),\n",
        "    random_state=42,\n",
        "    stratify=combined_df['classId']  # 클래스 비율 유지\n",
        ")\n",
        "\n",
        "temp_len = len(temp_df)\n",
        "valid_df, test_df = train_test_split(\n",
        "    temp_df,\n",
        "    train_size=int(len(combined_df) * 0.2),\n",
        "    random_state=42,\n",
        "    stratify=temp_df['classId']  # 클래스 비율 유지\n",
        ")\n",
        "\n",
        "train_df.reset_index(drop = True, inplace = True)\n",
        "valid_df.reset_index(drop = True, inplace = True)\n",
        "test_df.reset_index(drop = True, inplace = True)\n",
        "\n",
        "class args:\n",
        "    # amp = True\n",
        "    gpu = '0'\n",
        "\n",
        "    label_size = 3\n",
        "    epochs=10\n",
        "    start_epoch = 0\n",
        "    batch_size=8\n",
        "    weight_decay=1e-6\n",
        "    max_len = 60\n",
        "\n",
        "    start_lr = 2e-5\n",
        "    min_lr=1e-6\n",
        "\n",
        "    num_workers=0\n",
        "    seed=2022\n",
        "\n",
        "    path = '/content/drive/MyDrive/코딩/새싹해커톤'\n",
        "    save_model_path = '/whisper_tiny_cls.pt'\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
        "device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "od0ZeiZeWROk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46fc9754-6968-4fb1-ad3e-2dc7cea781ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_df = train_df[:100]\n",
        "# valid_df = valid_df[:20]"
      ],
      "metadata": {
        "id": "2TWkigge7SJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(len(train_df), len(valid_df), len(test_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQHARrvy8A1j",
        "outputId": "87f184b2-372c-4a99-e1c4-e2f804431d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "919 262 133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'openai/whisper-tiny'\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\n",
        "processor = WhisperProcessor.from_pretrained(model_name, language='Korean')\n",
        "tokenizer = WhisperTokenizer.from_pretrained(model_name, language='Korean')\n",
        "whisper_model = whisper.load_model(model_name.split('-')[-1])"
      ],
      "metadata": {
        "id": "unGmHu1qgIib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, df, feature_extractor, tokenizer, processor, seed):\n",
        "        self.df = df\n",
        "        self.feature_extractor = feature_extractor\n",
        "        self.tokenizer = tokenizer\n",
        "        self.processor = processor\n",
        "        self.seed = seed\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.df.iloc[idx]['audio_file_path']\n",
        "        audio, _ = librosa.load(file_path, sr=16000)\n",
        "        audio = whisper.pad_or_trim(audio.flatten())\n",
        "        mel = whisper.log_mel_spectrogram(audio)\n",
        "\n",
        "        input_features = torch.tensor(mel, dtype=torch.float32)\n",
        "        decoder_input_ids = torch.tensor(self.tokenizer.bos_token_id, dtype=torch.long)\n",
        "\n",
        "        labels = self.df.iloc[idx]['classId']\n",
        "        labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "        return {\"input_features\": input_features, \"labels\": labels, \"decoder_input_ids\": decoder_input_ids}\n",
        "\n",
        "train_dataset = CustomDataset(train_df, feature_extractor, tokenizer, processor, seed=42)\n",
        "train_loader = DataLoader(train_dataset, batch_size = args.batch_size, shuffle=True)\n",
        "valid_dataset = CustomDataset(valid_df, feature_extractor, tokenizer, processor, seed=42)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size = args.batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "umLjVT7BYqCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whisper_model = whisper.load_model(model_name.split('-')[-1])\n",
        "optimizer = torch.optim.AdamW(whisper_model.parameters(), lr=args.start_lr)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)"
      ],
      "metadata": {
        "id": "VCd6skSVmptq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, data_loader, loss_fn, opimizer, tokenizer):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    pred_list = []\n",
        "    target_list = []\n",
        "    pbar = tqdm(data_loader)\n",
        "    train_loss = 0\n",
        "    cer_score = 0\n",
        "\n",
        "    for i, batch in enumerate(pbar):\n",
        "        input_features = batch['input_features'].to(device)\n",
        "        labels = batch['labels'].long().to(device)\n",
        "        # decoder_input_ids = batch['decoder_input_ids'].long().to(device)\n",
        "        decoder_input_ids = batch['decoder_input_ids'].long().to(device).unsqueeze(1)\n",
        "        # print(decoder_input_ids.shape)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        audio_features = model.encoder(input_features).to(device)\n",
        "        outputs = model.decoder(decoder_input_ids, audio_features)\n",
        "\n",
        "        loss = loss_fn(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        opimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        pbar.set_description('\\033[1m[C_loss : {:>.5}]\\033[0m'.format(round(train_loss / (i+1), 4)))\n",
        "\n",
        "        pred = torch.argmax(outputs, dim=-1)\n",
        "        pred_list.extend(pred.cpu().numpy().tolist())\n",
        "        target_list.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "    train_loss = train_loss / len(data_loader)\n",
        "\n",
        "    pred_list = [p for batch_preds in pred_list for p in (batch_preds if isinstance(batch_preds, list) else [batch_preds])]\n",
        "    target_list = [t for batch_targets in target_list for t in (batch_targets if isinstance(batch_targets, list) else [batch_targets])]\n",
        "\n",
        "    # Accuracy & F1 계산\n",
        "    acc = accuracy_score(target_list, pred_list)\n",
        "    f1 = f1_score(target_list, pred_list, average='macro')  # 다중클래스면 macro\n",
        "    print(\"\\033[1m[ACC    : {:>.5}]\\033[0m\".format(round(acc, 4)))\n",
        "    print(\"\\033[1m[F1     : {:>.5}]\\033[0m\".format(round(f1, 4)))\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return model, train_loss / len(data_loader), acc, f1\n",
        "\n",
        "def valid(model, data_loader, loss_fn, tokenizer):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    pred_list = []\n",
        "    target_list = []\n",
        "    pbar = tqdm(data_loader)\n",
        "    valid_loss = 0\n",
        "    cer_score = 0\n",
        "\n",
        "    for i, batch in enumerate(pbar):\n",
        "        input_features = batch['input_features'].to(device)\n",
        "        labels = batch['labels'].long().to(device)\n",
        "        # decoder_input_ids = batch['decoder_input_ids'].long().to(device)\n",
        "        decoder_input_ids = batch['decoder_input_ids'].long().to(device).unsqueeze(1)\n",
        "\n",
        "        audio_features = model.encoder(input_features)\n",
        "        outputs = model.decoder(decoder_input_ids, audio_features)\n",
        "\n",
        "        loss = loss_fn(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
        "        valid_loss += loss.item()\n",
        "        pbar.set_description('\\033[1m[C_loss : {:>.5}]\\033[0m'.format(round(valid_loss / (i+1), 4)))\n",
        "\n",
        "        pred = torch.argmax(outputs, dim=-1)\n",
        "        pred_list.extend(pred.cpu().numpy().tolist())\n",
        "        target_list.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "    valid_loss = valid_loss / len(data_loader)\n",
        "\n",
        "    pred_list = [p for batch_preds in pred_list for p in (batch_preds if isinstance(batch_preds, list) else [batch_preds])]\n",
        "    target_list = [t for batch_targets in target_list for t in (batch_targets if isinstance(batch_targets, list) else [batch_targets])]\n",
        "\n",
        "    # Accuracy & F1 계산\n",
        "    acc = accuracy_score(target_list, pred_list)\n",
        "    f1 = f1_score(target_list, pred_list, average='macro')  # 다중클래스면 macro\n",
        "    print(\"\\033[1m[ACC    : {:>.5}]\\033[0m\".format(round(acc, 4)))\n",
        "    print(\"\\033[1m[F1     : {:>.5}]\\033[0m\".format(round(f1, 4)))\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return model, valid_loss / len(data_loader), acc, f1"
      ],
      "metadata": {
        "id": "8FgRVodug_gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_loss = 100\n",
        "\n",
        "for epoch in range(args.epochs):\n",
        "    if epoch < args.start_epoch:\n",
        "        continue\n",
        "\n",
        "    model, train_loss, train_acc, train_f1 = train(whisper_model, train_loader, loss_fn, optimizer, tokenizer)\n",
        "    model, valid_loss, valid_acc, valid_f1 = valid(whisper_model, valid_loader, loss_fn, tokenizer)\n",
        "\n",
        "    if valid_loss < save_loss:\n",
        "        save_loss = valid_loss\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "        }, args.path + args.save_model_path)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "# 4에폭 결과\n",
        "'''\n",
        "  0%|          | 0/115 [00:00<?, ?it/s]/tmp/ipython-input-1114513740.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
        "  input_features = torch.tensor(mel, dtype=torch.float32)\n",
        "[C_loss : 0.6985]: 100%|██████████| 115/115 [40:55<00:00, 21.35s/it]\n",
        "[ACC    : 0.7334]\n",
        "[F1     : 0.4079]\n",
        "  0%|          | 0/33 [00:00<?, ?it/s]/tmp/ipython-input-1114513740.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
        "  input_features = torch.tensor(mel, dtype=torch.float32)\n",
        "[C_loss : 0.3778]: 100%|██████████| 33/33 [12:44<00:00, 23.16s/it]\n",
        "[ACC    : 0.8015]\n",
        "[F1     : 0.663]\n",
        "  0%|          | 0/115 [00:00<?, ?it/s]/tmp/ipython-input-1114513740.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
        "  input_features = torch.tensor(mel, dtype=torch.float32)\n",
        "[C_loss : 0.3788]: 100%|██████████| 115/115 [33:23<00:00, 17.42s/it]\n",
        "[ACC    : 0.8313]\n",
        "[F1     : 0.79]\n",
        "  0%|          | 0/33 [00:00<?, ?it/s]/tmp/ipython-input-1114513740.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
        "  input_features = torch.tensor(mel, dtype=torch.float32)\n",
        "[C_loss : 0.3072]: 100%|██████████| 33/33 [10:16<00:00, 18.68s/it]\n",
        "[ACC    : 0.8511]\n",
        "[F1     : 0.8323]\n",
        "  0%|          | 0/115 [00:00<?, ?it/s]/tmp/ipython-input-1114513740.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
        "  input_features = torch.tensor(mel, dtype=torch.float32)\n",
        "[C_loss : 0.1996]: 100%|██████████| 115/115 [32:07<00:00, 16.76s/it]\n",
        "[ACC    : 0.8966]\n",
        "[F1     : 0.8756]\n",
        "  0%|          | 0/33 [00:00<?, ?it/s]/tmp/ipython-input-1114513740.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
        "  input_features = torch.tensor(mel, dtype=torch.float32)\n",
        "[C_loss : 0.3297]: 100%|██████████| 33/33 [09:20<00:00, 16.97s/it]\n",
        "[ACC    : 0.8473]\n",
        "[F1     : 0.835]\n",
        "  0%|          | 0/115 [00:00<?, ?it/s]/tmp/ipython-input-1114513740.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
        "  input_features = torch.tensor(mel, dtype=torch.float32)\n",
        "[C_loss : 0.1186]: 100%|██████████| 115/115 [31:29<00:00, 16.43s/it]\n",
        "[ACC    : 0.9402]\n",
        "[F1     : 0.9296]\n",
        "  0%|          | 0/33 [00:00<?, ?it/s]/tmp/ipython-input-1114513740.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
        "  input_features = torch.tensor(mel, dtype=torch.float32)\n",
        "[C_loss : 0.2733]: 100%|██████████| 33/33 [09:56<00:00, 18.07s/it]\n",
        "[ACC    : 0.8893]\n",
        "[F1     : 0.8598]\n",
        "'''"
      ],
      "metadata": {
        "id": "CBKKRphsz-w5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = whisper.load_model(\"tiny\").to(device)\n",
        "ckpt = torch.load(\"/content/drive/MyDrive/코딩/새싹해커톤/whisper_tiny_cls.pt\", map_location=device)\n",
        "model.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "nQx35MaWY2mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_audio(model, audio_path):\n",
        "    audio, _ = librosa.load(audio_path, sr=16000)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(device)\n",
        "\n",
        "    audio_features = model.encoder(mel.unsqueeze(0))\n",
        "\n",
        "    bos = tokenizer.bos_token_id\n",
        "    decoder_input_ids = torch.tensor([[bos]], device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model.decoder(decoder_input_ids, audio_features)\n",
        "\n",
        "    # vocab 51864개 중 앞 3개만 선택\n",
        "    logits3 = logits[:, -1, :3]          # (1,3)\n",
        "    probs = F.softmax(logits3, dim=-1)   # (1,3)\n",
        "\n",
        "    return probs.detach().cpu().numpy().flatten()"
      ],
      "metadata": {
        "id": "fyNK39Pv8_ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for idx, row in test_df.iterrows():\n",
        "    path = row[\"audio_file_path\"]\n",
        "\n",
        "    probs = classify_audio(model, path)\n",
        "    formatted_probs = [round(float(p), 4) for p in probs]\n",
        "    percent_probs = [round(float(p) * 100, 2) for p in formatted_probs]\n",
        "    print(formatted_probs)\n",
        "    print(percent_probs)\n",
        "    pred = int(np.argmax(probs))\n",
        "\n",
        "    results.append({\n",
        "        \"audio_file_path\": path,\n",
        "        \"p0\": percent_probs[0],\n",
        "        \"p1\": percent_probs[1],\n",
        "        \"p2\": percent_probs[2],\n",
        "        \"pred\": pred\n",
        "    })\n",
        "    print(pred)\n",
        "\n",
        "test_results = pd.DataFrame(results)\n",
        "test_results"
      ],
      "metadata": {
        "id": "Mnlmkbq4-gra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PTvS8fPf_IA4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}