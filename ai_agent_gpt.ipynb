{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JE0xcooFVo4",
        "outputId": "dafb7834-6443-45fd-dc2d-8418e9b996a2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.3.0 langchain-community==0.3.0 langchain-core==0.3.0 langchain-openai langchain-text-splitters faiss-cpu"
      ],
      "metadata": {
        "id": "K_Qc5zczvFFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 재시작하고 실행하기\n",
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "id": "ZpRugVHjomVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import json, os\n",
        "\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.tools import tool\n",
        "from langchain_core.messages import AIMessage, ToolMessage\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.tools import tool\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "from typing import Any, List, Optional, Dict\n",
        "from langchain_core.caches import BaseCache\n",
        "from langchain_core.callbacks import Callbacks\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
        "\n",
        "import torch\n",
        "import whisper\n",
        "from transformers import WhisperModel, WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
        "import librosa\n",
        "import torch.nn.functional as F\n",
        "\n",
        "try:\n",
        "    ChatOpenAI.model_rebuild()\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "api_key = userdata.get(\"OPEN_AI\")\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "fMfNl-7TP65m"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tool\n",
        "# 1. ASR\n",
        "# 2. 병 분리\n",
        "# 3. RAG"
      ],
      "metadata": {
        "id": "-t_zYHvdb3ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tool\n",
        "1. 화자 분리 ASR\n",
        "2. 병 구분\n",
        "3. 화자 텍스트 + 병 구분 + 자가진단표 정보로 정상, 관찰, 주의, 위험 구분\n",
        "4. 리포트 작성"
      ],
      "metadata": {
        "id": "SL02njW4w5e_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 화자 분리\n",
        "- 화자 분리와 타임 스탬프 asr에 담는다\n",
        "- 사용 모델: gpt-4o-transcribe-diarize\n",
        "- 반환값은 연속하여 작성"
      ],
      "metadata": {
        "id": "qR07sCvowJoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 화자 분리 ASR\n",
        "import json\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def diarized_transcription_tool(audio_path: str) -> dict:\n",
        "    \"\"\"\n",
        "    오디오 파일 경로를 받아 텍스트로 변환(ASR)합니다.\n",
        "    화자 분리 없이 전체 대화 내용을 텍스트로 반환합니다.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(audio_path, \"rb\") as audio_file:\n",
        "            # 가장 안정적인 기본 모델 사용 (whisper-1)\n",
        "            transcript = client.audio.transcriptions.create(\n",
        "                model=\"gpt-4o-transcribe\",\n",
        "                file=audio_file,\n",
        "                response_format=\"text\"  # 복잡한 JSON 대신 텍스트만 바로 받습니다.\n",
        "            )\n",
        "\n",
        "        # transcript는 문자열(str)로 반환됩니다.\n",
        "        return {\"text\": transcript}\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ASR Tool Error: {e}\")\n",
        "        return {\"text\": f\"음성 인식 실패: {str(e)}\"}\n",
        "\n",
        "# transcript는 sp0, sp2 이렇게 나오고 나중에 transcript를 몽땅 넣을 때는 둘 중에 한 명은 보호자, 피보호자이며, 대화의 내용에 뇌졸중, 치매, 파킨슨병, 정상 중 하나의 증상을 피보호자가 갖고 있으니까 이 점을 고려하라고 해야 할 듯"
      ],
      "metadata": {
        "id": "0NQMrpExcWpj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 병 분류\n",
        "1. 사용 모델: 학습시킨 openai/whisper-tiny (편리상 classifier layer없이 구분)\n",
        "2. 정확도: 88% 이상\n",
        "3. 구분: 뇌졸중, 기타 및 복합 퇴행성 뇌질환, 정상\n",
        "4. 출력: 리스트에 [뇌졸중 확률, 퇴행성 뇌질환 확률, 정상 확률]로 출력 (모두 float으로 소수점 4자리까지 출력되도록 함)"
      ],
      "metadata": {
        "id": "PZGwxQJZxQfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 불러오기\n",
        "model_name = 'openai/whisper-tiny'\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\n",
        "processor = WhisperProcessor.from_pretrained(model_name, language='Korean')\n",
        "tokenizer = WhisperTokenizer.from_pretrained(model_name, language='Korean')\n",
        "whisper_model = whisper.load_model(model_name.split('-')[-1])\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = whisper.load_model(\"tiny\").to(device)\n",
        "ckpt = torch.load(\"/content/drive/MyDrive/코딩/새싹해커톤/whisper_tiny_cls.pt\", map_location=device)\n",
        "model.load_state_dict(ckpt[\"model_state_dict\"], strict=False)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "def classify_audio(audio_path):\n",
        "    global model\n",
        "    audio, _ = librosa.load(audio_path, sr=16000)\n",
        "    audio = whisper.pad_or_trim(audio)\n",
        "    mel = whisper.log_mel_spectrogram(audio).to(device)\n",
        "\n",
        "    audio_features = model.encoder(mel.unsqueeze(0))\n",
        "\n",
        "    bos = tokenizer.bos_token_id\n",
        "    decoder_input_ids = torch.tensor([[bos]], device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model.decoder(decoder_input_ids, audio_features)\n",
        "\n",
        "    # 앞 3개만 선택\n",
        "    logits3 = logits[:, -1, :3]          # (1,3)\n",
        "    probs = F.softmax(logits3, dim=-1)   # (1,3)\n",
        "\n",
        "    probs = probs.detach().cpu().numpy().flatten()\n",
        "    formatted_probs = [round(float(p), 4) for p in probs]\n",
        "    percent_probs = [round(float(p) * 100, 2) for p in formatted_probs]\n",
        "\n",
        "    return percent_probs"
      ],
      "metadata": {
        "id": "Yt5J4PuHGt12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def classify_neuro_status_tool(audio_path: str) -> dict:\n",
        "    '''\n",
        "    질병 분류\n",
        "    '''\n",
        "    probs = classify_audio(audio_path)\n",
        "    return {\"accuracy\": probs}"
      ],
      "metadata": {
        "id": "9_eXPMF2dOTa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. RAG\n",
        "1. 랭체인 기반 RAG\n",
        "2. 임베딩 모델: text-embedding-3-small\n",
        "3. 역할: 문서에서 적절한 정보를 찾고, 이에 대한 일반인이 이해하기 쉬운 설명 제공\n",
        "4. 설명 시 사용 모델: gpt-4o-mini"
      ],
      "metadata": {
        "id": "ZXx86XYCxvoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiseaseRAG:\n",
        "    \"\"\"LangChain 기반 질병 정보 RAG 클래스\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        self.api_key = api_key\n",
        "        # OpenAI 임베딩 모델 사용\n",
        "        self.embeddings = OpenAIEmbeddings(\n",
        "            model=\"text-embedding-3-small\",\n",
        "            openai_api_key=api_key\n",
        "        )\n",
        "        self.vectorstore = None\n",
        "        self.llm = ChatOpenAI(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            temperature=0.7,\n",
        "            openai_api_key=api_key\n",
        "        )\n",
        "\n",
        "    def load_and_index_document(self, file_path: str, chunk_size: int = 500, chunk_overlap: int = 100):\n",
        "        \"\"\"\n",
        "        문서를 로드하고 청크로 분할한 뒤 FAISS 벡터스토어에 인덱싱\n",
        "\n",
        "        Args:\n",
        "            file_path: 의료 문서 txt 파일 경로\n",
        "            chunk_size: 청크 크기 (문자 수)\n",
        "            chunk_overlap: 청크 간 겹치는 부분 (문자 수)\n",
        "        \"\"\"\n",
        "        # 문서 로드\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        # 텍스트 분할기 설정\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "        # 텍스트를 청크로 분할\n",
        "        chunks = text_splitter.split_text(text)\n",
        "\n",
        "        # FAISS 벡터스토어 생성 및 인덱싱\n",
        "        self.vectorstore = FAISS.from_texts(\n",
        "            texts=chunks,\n",
        "            embedding=self.embeddings\n",
        "        )\n",
        "\n",
        "        print(f\"문서 인덱싱 완료: {len(chunks)}개 청크\")\n",
        "        return len(chunks)\n",
        "\n",
        "    def search_and_simplify(self, query: str, k: int = 3):\n",
        "        \"\"\"\n",
        "        쿼리로 관련 문서를 검색하고 GPT로 쉬운 설명 생성\n",
        "\n",
        "        Args:\n",
        "            query: 검색 쿼리 (질병명 등)\n",
        "            k: 검색할 문서 개수\n",
        "\n",
        "        Returns:\n",
        "            보호자가 이해하기 쉬운 설명 텍스트\n",
        "        \"\"\"\n",
        "        if self.vectorstore is None:\n",
        "            return \"RAG가 초기화되지 않았습니다.\"\n",
        "\n",
        "        # 유사도 검색으로 관련 문서 가져오기\n",
        "        docs = self.vectorstore.similarity_search(query, k=k)\n",
        "        retrieved_text = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "        # 프롬프트 템플릿 정의\n",
        "        prompt_template = \"\"\"다음은 '{query}'에 대한 의료 문서에서 검색한 내용입니다.\n",
        "이 내용을 보호자가 이해하기 쉬운 한국어 설명으로 바꿔 주세요.\n",
        "\n",
        "### 검색된 내용:\n",
        "{context}\n",
        "\n",
        "### 요구사항:\n",
        "1. 전문 용어를 일상 언어로 바꾸기\n",
        "2. 간단하고 명확하게 설명\n",
        "3. 중요 정보는 포함하되 너무 길지 않게\n",
        "4. 친근하고 안심시키는 톤으로 작성\n",
        "5. 2-3문단 정도로 간결하게\n",
        "\n",
        "### 양식 예시:\n",
        "뇌혈관이 막히거나 터져서 생기는 질환으로, 말이 꼬이거나 한쪽이 약해지는 증상이 동반될 수 있습니다.\n",
        "평소 고혈압, 당뇨, 심장질환이 있으면 위험이 높아집니다.\n",
        "자가진단표의 혈압/어지럼 응답, 한쪽 팔이 마비됨 문항  체크 때문에 높음으로 평가되었습니다.\n",
        "\n",
        "###설명: \"\"\"\n",
        "\n",
        "        prompt = PromptTemplate(\n",
        "            template=prompt_template,\n",
        "            input_variables=[\"query\", \"context\"]\n",
        "        )\n",
        "\n",
        "        # LLM으로 쉬운 설명 생성\n",
        "        formatted_prompt = prompt.format(query=query, context=retrieved_text)\n",
        "        response = self.llm.invoke(formatted_prompt)\n",
        "\n",
        "        return response.content\n",
        "\n",
        "    def get_qa_chain(self):\n",
        "        \"\"\"\n",
        "        RetrievalQA 체인 반환 (선택적 사용)\n",
        "        \"\"\"\n",
        "        if self.vectorstore is None:\n",
        "            raise ValueError(\"벡터스토어가 초기화되지 않았습니다.\")\n",
        "\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "        )\n",
        "\n",
        "        return qa_chain"
      ],
      "metadata": {
        "id": "m_42KL9_vLeZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_rag_instance = None\n",
        "# rag용 의료 문서\n",
        "# 주소 변경 필\n",
        "\n",
        "\n",
        "def initialize_disease_rag(api_key: str, document_path: str):\n",
        "    \"\"\"\n",
        "    RAG 초기화 및 문서 인덱싱 (노트북 시작 시 한 번만 실행)\n",
        "\n",
        "    Args:\n",
        "        api_key: OpenAI API 키\n",
        "        document_path: 의료 문서 txt 파일 경로\n",
        "    \"\"\"\n",
        "    global _rag_instance\n",
        "    _rag_instance = DiseaseRAG(api_key=api_key)\n",
        "    _rag_instance.load_and_index_document(document_path)\n",
        "    print(\"RAG 시스템 준비 완료\")"
      ],
      "metadata": {
        "id": "UVxUwU2VvLbh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def retrieve_disease_info_tool(query: str) -> dict:\n",
        "    \"\"\"\n",
        "    특정 질병에 대한 의학 문서 컨텍스트를 RAG 방식으로 가져옵니다.\n",
        "    질병명이나 증상을 입력하면 OpenAI 임베딩으로 관련 정보를 검색하고,\n",
        "    GPT로 보호자가 이해하기 쉬운 설명을 생성합니다.\n",
        "\n",
        "    Args:\n",
        "        query: 질병명 또는 검색 쿼리 (예: \"뇌졸중\", \"파킨슨병\", \"치매\", \"루게릭병\")\n",
        "\n",
        "    Returns:\n",
        "        보호자가 이해하기 쉬운 설명이 담긴 딕셔너리\n",
        "    \"\"\"\n",
        "    global _rag_instance\n",
        "    context = _rag_instance.search_and_simplify(query, k=3)\n",
        "\n",
        "    return {\"context\": context}"
      ],
      "metadata": {
        "id": "dIHeycD6eIsa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 백에서 받아야 하는 거: 자가문단표, 음성, 환자 정보\n",
        "# 보내야 하는 거: 백으로 출력값"
      ],
      "metadata": {
        "id": "9tqo1mZHGGx2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. AI Agent\n",
        "1. funciton calling\n",
        "2. 프롬프트\n",
        "3. json 형식으로 전체 결과 출력되도록\n",
        "4. ai agent: 병에 대한 위험도 판단(뇌졸중, 치매, 파킨슨병, 루게릭병)"
      ],
      "metadata": {
        "id": "RQgTG7Ky36uJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. function calling\n",
        "agent_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\n",
        "        \"system\",\n",
        "        \"\"\"\n",
        "당신은 뇌졸중 치매, 파킨슨병, 루게릭병을 평가하는\n",
        "AI 의료 보조 에이전트이다.\n",
        "\n",
        "### 사용 가능한 tool:\n",
        "- diarized_transcription_tool(audio_path): ASR\n",
        "- classify_neuro_status_tool(audio_path): 음성을 기준으로 뇌질환을 판별\n",
        "- retrieve_disease_info_tool(query): 특정 질병에 대한 의학 문서 컨텍스트를 RAG 방식으로 가져옴\n",
        "\n",
        "### 최종 목적:\n",
        "사용자가 제공한 정보(음성 파일 경로, 자가 문진표 정보)를 바탕으로\n",
        "다음과 같은 Python 딕셔너리 형태의 JSON을 생성하는 것이다.\n",
        "\n",
        "result = {{\n",
        "  \"accuracy\": [float(뇌졸중 확률), float(퇴행성 뇌질환 확률), float(문제 없음 확률)],\n",
        "  \"ASR\": \"통화 전사 데이터\",\n",
        "  \"risk\": [\"뇌졸중 위험도\", \"치매 위험도\", \"파킨슨병 위험도\", \"루게릭병 위험도\"],\n",
        "  \"explain\": [\"뇌졸중 설명\", \"치매 설명\", \"파킨슨병 설명\", \"루게릭병 설명\"]\n",
        "}}\n",
        "\n",
        "### 제약 조건:\n",
        "- \"accuracy\"는 classify_neuro_status_tool 툴의 결과를 그대로 사용한다.\n",
        "- \"ASR\"에는 diarized_transcription_tool을 사용해 얻은 전체 결과를 절대 요약하거나 내용을 변경하지 않은 채로 넣는다.\n",
        "- \"risk\" 리스트는 반드시 길이 4이며, 순서는 [뇌졸중, 치매, 파킨슨병, 루게릭병] 이다.\n",
        "- 각 위험도 값은 \"정상\", \"관찰\", \"주의\", \"위험\" 중 하나여야 한다. 이때 판단은 accuracy, ASR, 자가문단표를 기준으로 판단해야 한다.\n",
        "- \"explain\" 리스트는 길이 4이며, 순서 역시 [뇌졸중, 치매, 파킨슨병, 루게릭병] 이다.\n",
        "- 각 설명은 보호자가 이해하기 쉬운 한국어로 작성한다.\n",
        "- 만약 해당 질병 위험도가 \"정상\"인 경우에는 설명은 작성하지 않는다.\n",
        "- 최종 응답은 반드시 위 result 딕셔너리 형태와 동일한 구조의 JSON 객체로만 출력한다.\n",
        "  그 외의 텍스트(설명, 사족)는 출력하지 않는다.\n",
        "\n",
        "### tool을 사용할 때:\n",
        "1) 문단표 정보를 통해 현 상태에 대한 정보를 받는다. 문단표의 점수는 1~5 사이로, 1은 전혀 그렇지 않다, 5는 매우 그렇다를 나타낸다.\n",
        "2) 그 다음 classify_neuro_status_tool으로 세 가지 범주 확률을 얻는다.\n",
        "3) diarized_transcription_tool로 보호자와 피보호자의 대화 정보를 얻는다.\n",
        "4) 문단표 정보, 2번의 세 가지 범주 확률, 보호자와 피호자의 대화 내용을 기준으로 뇌졸중, 치매, 파킨슨병, 루게릭병에 대한 위험도를 정상, 관찰, 주의, 위험으로 각각 판단한다.\n",
        "5) 관찰 이상 단계의 병에 대한 정보를 retrieve_disease_info_tool을 사용해서 각 질병에 대한 설명을 보완하여 보호자에게 전달할 설명을 구성한다. 이때, 자가문단표의 내용을 그대로 출력하기 보다 보호자가 인지하고 있어야 할 내용이나 보호자가 수행해야 할 내용을 중심으로 출력한다. 출력 시, 마침표와 쉼표만 특수문자로 사용한다. 문장 종결 시, 마침표를 사용하며, 쉼표는 문장 내에서만 사용한다.\n",
        "\"\"\"\n",
        "    ),\n",
        "    (\n",
        "        \"user\",\n",
        "        \"\"\"\n",
        "다음 정보를 바탕으로 result를 생성해 주세요. self_report는 질병에 적합한 질문과 그에 대한 정도를 반영한 숫자가 포함된 정보입니다.\n",
        "\n",
        "- audio_path: {audio_path}\n",
        "- self_report(JSON): {self_report_json}\n",
        "\n",
        "위 audio_path를 사용해 tools를 호출해서 ASR을 생성하고 분류 확률을 계산하고,\n",
        "자가 문진표 정보를 반영하여 최종 result를 만드세요.\n",
        "\"\"\"\n",
        "    ),\n",
        "    # [수정 3] Agent가 생각할 공간(Scratchpad) 추가 (이거 없으면 ValueError 남)\n",
        "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "])\n",
        "\n",
        "# 에이전트 생성\n",
        "# (tools 리스트는 위에서 이미 정의되어 있다고 가정합니다)\n",
        "tools = [\n",
        "    diarized_transcription_tool,\n",
        "    classify_neuro_status_tool,\n",
        "    retrieve_disease_info_tool,\n",
        "]\n",
        "\n",
        "agent_llm = ChatOpenAI(\n",
        "    # [수정 4] 존재하지 않는 모델명(gpt-4.1-nano) -> gpt-4o-mini로 변경\n",
        "    model=\"gpt-5.1\",\n",
        "    temperature=0.7,\n",
        "    openai_api_key=api_key,\n",
        ")\n",
        "\n",
        "agent = create_tool_calling_agent(agent_llm, tools, agent_prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
        "\n",
        "\n",
        "# 파이프라인\n",
        "def run_agent_with_function_calling(audio_path: str, self_report: dict) -> dict:\n",
        "    \"\"\"\n",
        "    LangChain tool-calling 에이전트를 사용하여 전체 파이프라인을 수행하고,\n",
        "    최종적으로 result 딕셔너리를 반환한다.\n",
        "    \"\"\"\n",
        "    user_input = {\n",
        "        \"audio_path\": audio_path,\n",
        "        \"self_report_json\": json.dumps(self_report, ensure_ascii=False),\n",
        "    }\n",
        "\n",
        "    # agent 실행\n",
        "    output = agent_executor.invoke(user_input)\n",
        "\n",
        "    # agent의 최종 출력 가져오기\n",
        "    raw = output.get(\"output\", output)\n",
        "\n",
        "    # [수정 5] 결과 파싱 강화 (Markdown 코드 블록 제거 기능 추가)\n",
        "    if isinstance(raw, str):\n",
        "        # LLM이 ```json ... ``` 형태로 줄 경우를 대비해 태그 제거\n",
        "        clean_raw = raw.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "        try:\n",
        "            result = json.loads(clean_raw)\n",
        "        except Exception:\n",
        "            print(\"JSON 파싱 실패. 원본 출력:\", raw)\n",
        "            raise ValueError(f\"JSON 파싱 실패: {raw}\")\n",
        "    else:\n",
        "        # 혹시 dict로 바로 나왔다면 그대로 사용\n",
        "        result = raw\n",
        "\n",
        "    # 필수 키 검증\n",
        "    required_keys = [\"accuracy\", \"ASR\", \"risk\", \"explain\"]\n",
        "    for k in required_keys:\n",
        "        if k not in result:\n",
        "            raise ValueError(f\"결과에 '{k}' 키가 없습니다. 실제 결과: {result}\")\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "ruF7ndGBrZ0l"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. RAG 초기화 (한 번만 실행)\n",
        "initialize_disease_rag(api_key=api_key, document_path=\"/content/drive/MyDrive/코딩/새싹해커톤/rag_practicce.txt\")\n",
        "\n",
        "# 2. 에이전트 실행\n",
        "# audio_path = \"/content/drive/MyDrive/코딩/새싹해커톤/일반남여_일반통합06_F_1536505292_35_경상_실내_06320.wav\"\n",
        "audio_path = \"/content/drive/MyDrive/코딩/새싹해커톤/talk_set2_collectorgs384_speakergs4917_speakergs4918_6_0_120.wav\"\n",
        "self_report = {\n",
        "    \"한쪽 얼굴이 둔하고 손발이 저리거나 힘이 빠진다\": 1,\n",
        "    \"한쪽 손에 힘이 없어 물건을 떨어뜨리거나 다리가 후들거려 비틀거린다\": 1,\n",
        "    \"어떤 일이 언제 일어났는지 기억하지 못할 때가 있다\": 2,\n",
        "    \"며칠 전에 들었던 이야기를 잊는다\": 2,\n",
        "    \"손을 움직이거나 가만히 있을 때도 손이 떨린다\": 1,\n",
        "    \"침대나 의자에서 일어날 때 몸이 무겁고 힘들다\": 2,\n",
        "    \"옷 단추를 잠그거나 물건을 잡기 힘들다\": 1,\n",
        "    \"근육 경련이 일어난다\": 2\n",
        "}\n",
        "\n",
        "result = run_agent_with_function_calling(audio_path, self_report)\n",
        "print(json.dumps(result, ensure_ascii=False, indent=2))"
      ],
      "metadata": {
        "id": "MRUKWZST_aiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S3Sv2op4r5vi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}